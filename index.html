<!DOCTYPR html>
<html lang = "zh-CN">
<head>

<meta charset = "utf-8">
<meta name="author" content="Haonan He">
<link rel="stylesheet" type="text/css" href="mystyle.css">
<meta name="description" content="Haonan He 何昊南">
<title>Haonan He 何昊南</title>
<link rel="shortcut icon" href="figure.ico" type="images/x-icon">
</head>

<body style="width:800px; margin:0 auto;"></body>
<table border="0">
  <tr>
    <td width="25%">
      <img src="images/figure.png" width="10" height="10">
    </td>
    <td width="75%", style="padding:5%;width:63%;vertical-align:middle" >
      <h1>Haonan He 何昊南</h1>
      <p>auhaonanhe@mail.scut.edu.cn</p>
      <p>(+86)199-275-27314</p>
      <p><b>I am currently an undergraduate student at the School of Automation Science and Technology，South University of Technology.</b></p>
      <br>
      <p><b>My reasearch interest lies in machine learning, computational neuronscience and transfer learning. I am interestd in developing machine learning models to decode information contained in
        neuronal data. And I believe machine learning can help us better understand how neuron populations encode physical variables. I am also interested in applying transfer learning
        techniques to solve the problem of lack of labeled data.</b></p>
      <br>
      <p> <a href="https://haonanhe.github.io/cv"> Curriculum Vitae </a> / <a href="https://github.com/haonanhe"> GitHub </a> / <a href="www.linkedin.com/in/haonanhe"> LinkedIn </a> </p> 
      </p>
    </td>
  </tr>
</table>

<div>

<h2>
Research Experience
</h2>
<table border="0", style="font-size:16px;">
  <tr>
    <td style="width:70%">
    <p style="text-indent:2em">Since Mar 2021, I started to apply machine learning methods to neural decoding under the supervision of Professor Pengcheng Zhou from Shenzhen Institute of Advanced Technology.
      We developed two Transformer-based models to decode spike trains and examined Transformer's superior ability of dealing with large-scale long-range dependencies. We also introduced a GLM-based data 
      augmentation method for overcoming the data shortage issue. Using the self-attention structure in Transformer, we visualized the inter-neuron information transmission activities. The related manuscript is under review.</p>
    <p style="text-indent:2em">Before that, I worked in the Center for Brain Computer Interfaces and Brain Information Processing under the supervision of Professor Tianyou Yu from South China University of Technology.
      Our researches focused on solving the data shortage problem of EEG signals. We tried to use domain adaptation methods to transfer EEG signals between subjects. We also applied GAN to generate synthetic EEG for data augmentation. </p>
    </td>
  </tr>
  

</table>
</div>

<div>
<h2>
Punlication
</h2>
<table border="0">
  <tr>
    <td style="width:70%">
    <p style="font-size:16px;"><b>Spatial-Temporal Transformer-based Methods for Neural Decoding, under review (first author)</b></p>
    </td>
  </tr>
</table>
</div>

<div>
<h2>
Miscellaneous
</h2>
<table border="0">
  <tr>
    <td style="width:70%">
    <p style="font-size:16px;"><b>Spatial-Temporal Transformer-based Methods for Neural Decoding, under review (first author)</b></p>
    </td>
  </tr>
</table>
</div>

</body>

<html>
